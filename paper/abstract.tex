\documentclass[10pt,a4paper]{article}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{url}
\usepackage{color}
\usepackage{listings}

\author{Amar Kadamalakunte, Meghana Sreekanta Murthy,\\Giuseppe Rizzo, Rapha\"el Troncy}
\title{Spotting and Disambiguating Events\\from Social Live Streams}
%\title{EventSpotter: Spotting entities that talk about events}

\begin{document}
\maketitle
\section{Introduction}
\label{sec:introduction}
Social event sharing websites such as last.fm, Eventful, Eventbrite, and Facebook contain the most precise and up-to-date information related to scheduled or planned events. 
%The dynamic nature of these platforms allows to have a continuos stream. 
Generally these events are characterized by the surface form (event name), and additional features such as category, location, time, and name of agents (who perform at the event). Therefore, "spotting an event" means traversing a multi-dimensional space, labelled by a clear ambiguous definition given by the event name. 
Historically, different approaches have narrowed down the event extraction task to that of entity extraction. However, event extraction can hardly be passed off as "just another" entity extraction task. A closer examination reveals that the complexity of the extraction task increases manifold in the event domain. The biggest challenge of this task is to obtain a clear unambiguous definition of an event name because there is a lack of uniform rule based nomenclature with respect to events. 
%{\color{red}{G:need to cut off the many examples, which are verbose}}\newline
For instance, it is common practice for musical events to be named after the artists who are performing at the event. 
%Here, the event title is ``Beth Jeans Houghton'' which is also the name of the performing artist. 
This brings in a word sense disambiguation problem, where the onus is on comprehending whether the spot refers to the artist or to the event. The next level of complexity is introduced when an event title is created as an amalgamation of the artist names, band names, event venue and at times even the date, using special characters. %Here, ``Stay Sharp Album Release Party: Higher Hands + Grilled Lincolns + Pressing Strings'' is an event title as per the information uploaded on the event sharing website ``eventful''. 
In essence this means any stripping or stop-wording of input text, which would have otherwise been a natural choice for a preprocessing step in any information extraction problem, is completely out of question. This also means the length of an event title may range from just one word to a phrase with many words and punctuations. Further, at times, the boundaries of creative license are stretched to the point where events are named with words that do not even exist in the language vocabulary. 
%Here, ``Pocktoberfest'' is an yearly event which takes place at the ``Pocklington arts center''. But the word ``Pocktoberfest'' does not exist in the English language. 
%If all events followed a standard naming scheme, our task which now seems herculean, would have been average, at best. 
Finally, the brief and fleeting nature of events shared on social platforms also adds to the complexity of this task, which proves to be a challenging playground for text analysis tools that are often tuned to longer and more stable texts~\cite{ERP:2013}.
The live nature of event detection makes the extraction process further complex. This results in partial lack of a benchmark to start a canonical natural language chain. In this work we explore the plausibility of using a mixture of various information extraction and semantic web techniques for event detection and demonstrate that it is possible to obtain an increase in recall even with a linear combination of linguistic and knowledge based approaches grounded on a live stream of social event data in order to spot and disambiguate event entities in plain text. Both the source code of the experiments and the benchmark proposed in this work are publicly available at \url{http://github.com/amark-india/eventspotter}.

\section{Approach}
\begin{figure}
\includegraphics[width=\textwidth]{architecture.pdf}
\caption{The architectural diagram of the proposed approach. Core stages of the approach are: the Eventmedia dataset, Indexer, Candidate Selection and Disambiguation. The framework accepts as input a plain text and serializes the output as JSON (following the NERD API ~\cite{RIZZO:2012} patterns) and a XML-like format as reported in the example.}
\label{fig:architecture}
\end{figure}
The live social event data stream is pivotal for our approach. We use the Eventmedia dataset~\cite{krouf2012}, which offers an always up-to-date window on events which are spread across different social platforms. We incrementally compute snapshots of the dataset and build our event lexicon indexer on top of it. This acts as an event knowledge base, a \textit{de-facto} bulk of the proposed work. Our approach is then composed of the following stages: \textit{i)} preprocessing, \textit{ii)} candidate selection and \textit{iii)} disambiguation. \textit{i)} is orthogonally applied for normalizing the Eventmedia events and for every textual input; the data is first tokenized, then converted to the CoNLL IOB format. To aid the second stage, we added five additional features to the dataset: capitalization information (initial capital, all caps), prefix (first three letters of the token), suffix (last three letters of the token), whether token starts with a digit.
\textit{ii)} checks for a case-insensitive match with the tokenized version of the indexer of events. 
%The reason for performing such a case insensitive match is to ensure that we encompass all candidate event names, without losing any because of an insignificant case-sensitive mismatch. We call this type of mismatch insignificant because of the fact that since we consider unstructured input text, it is quite possible that event names may not be highlighted consistently in the same manner of case. 
The aim is to start with the largest set of candidate events and gradually limit the set to confirmed events by disambiguation. 
\textit{iii)} ascertains the validity of each entry in the candidate list with the help of meta-data stored in the Eventmedia dataset. 
%The proven hypothesis is that for a knowledge based approach to event detection, to identify only those events which are reasonably described in the input text. We consider an event as reasonably described if and only if there is a mention of : at least one of the agents (performing artists) or  the location (event venue) in the input text. In other words, this means that many events may go undetected in cases where the agent or the location string is not present in the input text. Though this would affect the recall of the system, we felt that it would be a trade-off worth making. This is primarily because, a large number of events are titled with commonly used words in a language. This means that any strategy that adopts an approach that is less restrictive would end up with a many false positive results. 
%As mentioned earlier, we recognize that basing our approach solely on the first hypothesis, will not allow us to achieve a near-exhaustive detection of event entities in the text. Hence, we intend to deal with these false negatives by employing a linguistic approach in the second pass to detect those events which are not reasonably described in the input text. 
For each candidate event the Eventmedia normalized dataset is consulted to get names of agents who had performed at that event. The input text is then parsed to spot these agent tokens using the naive spotter described in the candidate selection stage. If at least one agent is found in the input text then the candidate spot is considered as a valid event and added to the list of detected events. For each detected event a cosine similarity measure,  between the surroundings of the spot and the event description, is computed. Event descriptions contain other key information such as location, sentiment expressions such as ``thrilling'', ``exciting'' and ``mind blowing'', entry fee, domain specific terms such as ``rock'', ``music'', ``concert'', some historical background about the artists, events and so on. These are aspects which are unique to each event and there is a mention of some or all of these attributes in each event description. By computing a cosine similarity with the event's description we manage to match various miscellaneous attributes of an event which can, generally, not be categorized into fields such as agents, and location. We empirically defined the surroundings to be the current, previous and next sentences from the spot. This cosine similarity measure defines the confidence score for each detected event. The disambiguation stage yields a list of detected event names and their respective confidence scores.
%{\color{red}{A+M: redo the figure, remove upcoming logo and add Eventbrite, Facebook logos}}
Figure~\ref{fig:architecture} shows the architectural diagram of the proposed approach named EventSpotter. 

%\begin{lstlisting}
%Input : unstructured text 
%Output : disambiguated musical events
%
%# Start the preprocessing by tokenizing input text
%tokens = tokenize(input)
%
%sentences = sentensize(input)
%
%#Candidate Selection
%candidates = doSpotting(input, tokens, entries)
%# entries is a list of tokens obtained by tokenizing the list of event titles file
%# candidates are case insensitive matches of tokens with entries
%
%#Disambiguation
%for each candidate in candidates
% if candidate doesn''t start with capital letter and first char is not a digit
%  then continue to next candidate
% end if
%
% List<String> agents = geatagents(eventId)
% Set<FeatureStructure> foundAgentToks = doSpotting(input, tokens, agents)	
%	
% if atleast one agent is found in the input text  
%  then get surroundings to compute confidence score		
%  Surround = getSurrounding(sentences,candidate);
%  #If candidate is in the first line of input text, current and next line is surrounding.
%  #If candidate is in the middle of input text, previous, current and next line is surrounding.
%  #If candidate is in the last line of input text, current and previous line is surrounding.
%
%  confidence = cosine similarity.run(Surround,event description)
%	 
%  confirm candidate as an event and add to FeatureStructure.
% endif	
%endfor
%#Post processing
% Add <EVENT> tags to input text 
% Display annotated input text 
% Display spotted events with confidence score in JSON format
%\end{lstlisting}


\section{Benchmark}{
%Even though one of the strengths of the EventSpotter approach was to rely on a continuously updated, dynamic, live event knowledge base, we used a static benchmark to perform a canonical evaluation to compare this approach with the state of the art systems. We created an in-house benchmark, which was supervised by 4 human raters who achieved a good agreement score. In this benchmark we assessed both NER and NEL tasks.
%To the best of our knowledge no corpus exists to assess the performances in a scenario as specific as ours. 
Performance assessment, in a scenario as specific as ours, mandates the presence of a clean and precise gold standard corpus for events and to the best of our knowledge no such corpus existed. We therefore randomly selected 60 different events, that all had English descriptions, from the Eventmedia dataset. Four human annotators participated in the task of analysing these event descriptions to spot musical events and to disambiguate them to Eventmedia entries. The overall agreement score they achieved was good. The benchmark thus created was composed of 115 event annotations and has been made available at \url{https://github.com/amark-india/eventspotter/blob/master/data/GoldenDataset/GS-manual.txt}. Table~\ref{tab:benchmark} reports the statistics of the dataset.

%The EventSpotter project makes use of a snapshot of the EventMedia to detect all musical events that occurred in 2012. Thus, it is obvious that the test corpus must contain only events that occurred in this year. To the best of our knowledge no such corpus existed and so we were faced with the ardous task of creating one. However, instead of starting from scratch we decided to use event descriptions from the EventMedia dataset and annotate them. Since event descriptions in the dataset were essentially texts uploaded by multiple users onto various event sharing sites, using them as test corpora would help simulate a real world test scenario. But the EventMedia dataset contains close to 65,000 entries and it was neither feasible nor necessary for the evaluation stage of our project to use all these event descriptions. Instead we selected 100 event descriptions at random, such that this test sample represented the entire EventMedia dataset in an apt and unbiased manner. Before annotating these documents we preprocessed them to ensure that all HTML tags and URLs were removed so as to facilitate the creation of a clean golden dataset. We further reduced the size of the corpus to 60 documents by selecting only those that were in written in English. These documents can be found at \url{https://github.com/amark-india/eventspotter/tree/master/data}. 

%For experimental purposes, we followed 2 types of annotation: synthetic annotation and manual annotation. In the case of synthetic annotation, we carried out a straightforward string comparison between the input text and all event titles in the EventMedia dataset. Every match was considered as a valid spot and was annotated as an event. Adopting this synthetic approach was not only aided in quickly establishing a base to assess the performance of the EventSpotter. But also, came in handy during the training phase of the Stanford CRF classifier. We will see during the presentation of the results that the Stanford CRF classifier performed quite differently with synthetically annotated training data when compared to manually annotated training data. Finally, we realised by observation, that some aspects of the synthetic annotation approach could be applied during the manual annotation process.

%As we performed manual annotation, we realised that there was an inherent ambiguity in event titles. We found it extremely difficult, at times, to distinguish between event titles and artist names. Mainly because events were often titled as a mash up of the artist names, venue and sometimes the date of occurrence. Due to this ambiguity many event titles were not annotated despite strict adherence to the annotation rules. Thus, it made sense to not rely solely on the annotators'' knowledge, but in fact, partially adopt the synthetic annotation approach in order to generate a more robust golden dataset. The manual annotation was broken down into two passes. In the first pass, an unbiased, rule based manual annotation was performed. In the second pass, the human annotators were given the list of event titles that were being described in the documents. With this posteriori knowledge the human annotators were able to annotate event occurrences which would have otherwise gone unnoticed. As human annotators, we leveraged the context of the spot to perform word sense disambiguation. We refrained from annotating those spots where in the event title string was not used to directly refer to the event itself. In the phrase ``Carrie Underwood announces her North American tour Blown Away'', ``Carrie Underwood'' referred to the artist, not the event title and hence was not annotated (see example 8.1). But in another instance, ``Carrie Underwood comes to town this Monday.'' since the annotator knew that Carrie Underwood was the event title and since this string was being used to refer to an event in this context, the spot was annotated as an event (see example 8.2). We observed that this hybrid approach towards manual annotation was effective in improving the golden dataset and this was corroborated by the improvement in performance results as presented in the following section.
\begin{table}[h]
\centering % used for centering table
\begin{tabular}{c c } % centered columns (4 columns)
\hline %inserts double horizontal lines
Description & Values \\ [0.5ex] % inserts table
%heading
\hline\hline % inserts single horizontal line
number of Eventmedia URIs & 60  \\
total number of tokens & 7277  \\
total number of tokens annotated by synthetic annotation & 122  \\
total number of tokens annotated by manual annotation & 115  \\
\hline %inserts single line
\end{tabular}
\caption{Statistics of the benchmark proposed in this paper.} % title of Table
\label{tab:benchmark} % is used to refer this table in the text
\end{table}

\section{Experiments and Results}
We performed 10-fold cross validation and found that the EventSpotter performed better than the Stanford NER in terms of recall and F-measure. Results are computed using the conlleval script\footnote{\url{http://www.cnts.ua.ac.be/conll2002/ner/bin/conlleval.txt}} and plotted using R.

As baselines we initially chose OpenCalais (as an off-the-shelf solution) and Stanford NER (version 3.2.0) properly trained with data from the training set. 
A long survey on off-the-shelf systems that are able to detect musical events uncovered an extremely specific domain, where systems are tuned to perform reasonably well on tailored data. This consideration was further proven by the results achieved and could also explain OpenCalais' inability to detect events of the same kind available in the Eventmedia dataset. For this reason, we chose to omit the OpenCalais results from this paper.
Instead we retrained the Stanford NER system on a portion of training data per folder, using parameters based on the \textbf{english.conll.4class.distsim.crf.ser.gz} properties file provided with the Stanford distribution.

%It obtained a score of 89.26\% for recall and 72.73\% for F-measure, over the scores of 4.27\% and 8\% respectively of the Stanford NER trained with synthetic annotation and 54.70\% and 60.95\% respectively when trained with manual annotation. To highlight once again, these set of tests on the synthetically annotated golden dataset were carried out merely to give ourselves an idea about the EventSpotter's performance. Opencalais did not identify any musical event entities at all. Opencalais detects events from various categories which does not include musical festival or concert. Since it does come close with the category ``musical bands'' we thought it would be interesting to note how many events it would detect.
%\begin{table}[ht]
%\centering % used for centering table
%\begin{tabular}{c c c c} % centered columns (4 columns)
%\hline\hline %inserts double horizontal lines
%Approach & Precision & Recall & F-Measure \\ [0.5ex] % inserts table
%%heading
%\hline % inserts single horizontal line
%EventSpotter & 61.36\% & \bf 89.26\% \bf & \bf 72.73\% \bf \\ % inserting body of the table
%Stanford trained on synthetic data & 62.5\% & 4.27\% & 8\%\\
%Stanford trained on manual data & \bf 68.82 \bf \% & 54.70\% & 60.95\% \\
%Opencalais & 0\% & 0\% & 0\% \\
%\hline %inserts single line
%\end{tabular}
%\caption{Tests Performed with Synthetically Annotated Golden Dataset with 122 valid event annotations} % title of Table
%\label{table:nonlin} % is used to refer this table in the text
%\end{table}
The results presented in Table~\ref{tab:manual} are expressed in percentage and show the performance of individual NER extractors on the benchmark data. For the sake of comparison, the results proposed consider only the NER task, omitting the disambiguation part, one of the key features of our work. EventSpotter outperforms the results achieved by Stanford NER. This is mainly due to all the threats we listed in Section~\ref{sec:introduction} which make it extremely hard for a pure NER chain. We then trained the Stanford NER with a synthetic dataset annotated using a Gazetteer approach, where the Eventmedia dataset served as lookup knowledge base. The Stanford NER performance was boosted significantly, performing 4 times better in terms of recall and f-measure. We also tested a linear combination of EventSpotter and Stanford NER trained on synthetically annotated data and observed a slight increase in recall. It shows that the entities extracted from both approaches are nearly the same, with an overall overlap of $91.08\%$.

%The second set of tests run were run on the manually annotated golden dataset which contained 115 valid event annotations. Again the EventSpotter's performance out-shined that of the Stanford CRF in terms of recall and F-measure. It obtained a score of 68.14\% for recall and 43.14\% for F-measure, over the scores of 54.87\% and 56.88\% respectively of the Stanford CRF trained with synthetic annotation and 11.5\% and 18.71\% respectively when trained with manual annotation. Again, Opencalais did not identify any musical event entities at all. We also tested a linear combination of EventSpotter and Stanford CRF trained on synthetically annotated data, with Stanford CRF acting as a gazetteer to EventSpotter. Though there wasn't a stark improvement, the recall score did increase slightly to 70.8\%.
\begin{table}[h]
\centering % used for centering table
\begin{tabular}{c c c c} % centered columns (4 columns)
\hline %inserts double horizontal lines
& p & r & F1 \\ [0.5ex] % inserts table
%heading
\hline\hline % inserts single horizontal line
Stanford NER & 50 & 11.5 & 18.71 \\
EventSpotter & 31.56 & 68.14 & 43.14 \\
\hline
Stanford NER (synthetic) & \textbf{59.05} & 54.87 & \textbf{56.88} \\
Stanford NER (synthetic) + EventSpotter & 31.25 & \textbf{70.8} & 43.36 \\
\hline %inserts single line
\end{tabular}
\caption{Precision (p), Recall (r) and F-measure (F1) results for Stanford NER, EventSpotter, Stanford NER trained with the synthetic dataset, and a combination of the Stanford NERD (synthetic) + EventSpotter on the benchmark data for the NER task.} % title of Table
\label{tab:manual} % is used to refer this table in the text
\end{table}

\section{Discussion and outlook}
In this work we have proposed an approach that exploits social platform live streams for spotting and disambiguating events enclosed in plain text. Using a mixture of natural language and semantic web techniques, the proposed approach outperforms the Stanford NER, retrained with the benchmark data, in the NER task. We have also proposed a slightly different approach towards training the Stanford NER, based on Gazetteers, thus improving the recall significantly while still maintaining a decent precision. Finally, a new benchmark created by four experts has been introduced and discussed. 

As of today the number of events that occur around the world has increased exponentially as compared to the pre-social Web revolution days when conventional media such as television, radio and print were used to
publicise events. The process of informing fans or the general public about an event has been greatly simplified to the extent that, all it takes is an update on a few popular social networking websites. This along with the emergence of on-line ticketing websites, has meant that the Web has become a source of an enormous amount of up-to-date and precise event data. Also, more and more event related data is generated as people speak about events on discussion forums, blogs and social networking websites. This presents a unique opportunity for event promoters to gauge the popularity of their events and identify their target audience based on popularity measures which show how much their events are being spoken about in the social domain. Systems, like the one presented in this paper can leverage live stream social data to spot events and precisely fulfil such needs to their entirety.

\bibliographystyle{plain}
\bibliography{eventspotter}
\end{document}