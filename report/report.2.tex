\documentclass[a4paper,11pt]{report}
\usepackage[latin1]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx} 
\usepackage{pdfpages}
\usepackage{fancyvrb}
\bibliographystyle{ieeetr}
\begin{document}

\begin{titlepage}
\begin{center}
\includegraphics[width=5cm]{EURECOM_logo_quadri}
\\[3cm]
\textbf{\Huge{Eventspotter}}
\\[0.2cm]
\textbf{\textsc{\LARGE{spotting entities that talk about events}}}
\\[2cm]
\textbf{\textsc{\LARGE{Project Report}}}
\\[0.5cm]
\LARGE{Amar Kadamalakunte , Meghana Sreekanta Murthy}
\\[0.1cm]
\large{Fall 2012 - Spring 2013}
\\[7.92cm]
\columnsep3cm
\begin{tabular}{p{8cm} p{8.5cm}}
\small{\textbf{Supervisors:}\newline
Rapha\"el Troncy\newline
Giuseppe Rizzo} 
&
\small{\textbf{EURECOM\newline Multimedia Department}}
\end{tabular}
\end{center}
\end{titlepage}


 \tableofcontents

\chapter*{1 Abstract}
\addcontentsline{toc}{chapter}{1 Abstract}
By definition, an event is an occurrence in the past, present or future. Events could be classified as musical events, sports events, news events, etc. The aim of our work is to develop an approach to spot the existence of musical event entities in free form text. The Eventspotter uses a large dataset called EventMedia to identify candidate event spots in the text, and disambiguates them by assigning a confidence score. The confidence score itself is based on the identification of correlated entities (event location or artists) in the surrounding text and cosine similarity measure between the surrounding text and existing description of event in the dataset. We used precision, recall and f-measure to evaluate our approach. We used event description from the Eventmedia dataset as input and found that the Eventspotter performed better than some exisiting approaches for event detection in the musical domain. We present our method and findings in this paper.

\chapter*{2 Introduction}
\addcontentsline{toc}{chapter}{2 Introduction}

%\section*{Motivation for the project}
%\section*{Contents}

\chapter*{3 Related Work}
Many people have tried to tackle the problem of entity detection in the past. While some have relied purely on the linguistic properties of text others have used context and domain knowledge to accurately detect entities. \cite{Gruhl_contextand} was one such approach that inspired us during the literature survey phase of the Eventspotter. In this case, the utilisation of context and domain knowledge along with the application of real world constraints proved to be beneficial in improving the accuracy of entity detection. The main goal was to establish semantic relationships between entities in social networking and content sharing sites like MySpace and Youtube. In this approach, the authors used an edit- distance based spotter to match the knowledge base with the text taken from online forums discussing popular music. They then checked for domain and context information and applied an SVM classifier. Further, a study of \cite{Hassell06ontology-drivenautomatic} introduced us to the approach of identifying entities based on knowledge from populated ontologies such as DBWorld and DBLP. They used clues such as entity names, text-proximity, text co-ocurrence, popularity of entities, semantic relationships between entities for disambiguation. This approach further vindicated our idea of using the large Eventmedia knowledge base to detect events in Eventspotter.\newline

As part of another approach , DBpedia URIs were used for automatic annotation of texts.  The DBPedia Spotlight \cite{Mendes11dbpediaspotlight:} used an extensive ontology with 272 classes, called DBpedia, and also provided the flexibility of choosing domain of interest.  2 sources of textual references include : the lexicon created from the DBPedia graph (of labels, redirects, disambiguations) and wikilinks. The Eventspotter derives various facets of the DBpedia Spotlight approach including the 4 main stages : (1)Spot phrases that may indicate a mention of a DBpedia  resource, (2)Select candidates by spotted phrase-to-resources mapping, (3)Disambiguate by using the context around spotted phrase, (4)Configure the annotation by customization from user with  Resource Prominence, Topic Pertinence, Contextual Ambiguity, Disambiguation Confidence.We also studied two interesting methodologies that were applied to social media and wikipedia for detection of events. TWICAL \cite{Ritter_opendomain} focused on linguistic properties and temporal expressions to detect events, classified them using the context of surrounding phrases and finally ranked them according to strong associations with a unique date. The Wikipedia Live Monitor\cite{DBLP:journals/corr/abs-1303-4702} tracked edits on various linguistic versions of a single document on wikipedia and relied on cross-language, full-text search across social networks in order to evaluate the \cite{Hassell06ontology-drivenautomatic} plausability of the edits being events. \newline

Although the above mentioned approaches are effective in entity spotting, there are a few limitations with each. In \cite{Gruhl_contextand}, the authors cite the absence of a training corpus and regions of text being commonly used words, as limiting factors. They  also speak of the ambiguous nature of song titles being a major limitation. In  \cite{Hassell06ontology-drivenautomatic} the non-exhaustive nature of any knowledge base proves to be the biggest hurdle. The same is the case with the DBpedia spotlight approach defined in \cite{Mendes11dbpediaspotlight:}, where the authors do not speak of using linguistic tools such as a Conditional Random Field classifier to detect entities that are non existent in the knowledge base.Both TWICAL \cite{Ritter_opendomain} and Wikipedia Live monitor \cite{DBLP:journals/corr/abs-1303-4702} are dependant on social networking sites to detect events. As a result, events that are not publicised in the social networking domain go undetected. The fact that each of these approaches have limitations of their own, goes to show that there is still scope for improvement in the field of event detection. The EventSpotter project is an attempt to meet this need for a more complete approach towards event detection. We first take a detailed look at the Eventspotter Approch and then in subsequent sections we speak at length about the golden data set creation, usage of 10-fold cross validation to evaluate the performance of the system and conclude with our thoughts on how we can potentially better the performance of the eventspotter with the help of machine learning techniques. 
\addcontentsline{toc}{chapter}{3 Related Work}


\chapter*{6 Evaluation}
We tested the Eventspotter against Stanford Conditional Random Field\footnote{\url{http://nlp.stanford.edu/software/CRF-NER.shtml}} and Opencalais\footnote{\url{http://www.opencalais.com/}}. These tests were of a relatively small scale due to the need to manually create the the golden dataset. We felt it would be interesting to understand the performance of the flexible knowledge based approach of the Eventspotter against the purely grammar-based approaches of Stanford and Opencalais. Before we present the results of our experiments, it would be appropriate to understand the creation of the golden dataset. 
\addcontentsline{toc}{chapter}{6 Evaluation}

\section*{Golden Dataset Creation}
We used 60 event descriptions for the creation of the golden dataset. 30 documents were known to include event titles as well as artist names while the remaining were chosen at random. We performed cross validation with factor k =10. We followed a set of rules in order to ensure consistent annotation for the golden dataset creation. For the purpose of annotation we defined a musical event title as : a ``proper noun that refers to a musical concert or festival''. In all cases, an agreement between 4 human taggers was necessary. Any disagreement was resolved after deliberation and discussion. A candidate spot, was tagged with \textless EVENT \textgreater  and \textless /EVENT\textgreater. For instance, if `scala' is an event in the phrase `scala:)', we annotate it as `\textless EVENT\textgreater scala \textless /EVENT\textgreater:)'. For experimental purposes, we followed 2 types of annotation : synthetic annotation and manual annotation . 
	In the case of synthetic annotation, we carried out a straightforward string comparison between the input text and all event titles in the Eventmedia dataset. If there was a match, the spot was annotated as an event. There were several reasons for doing so. First, we were able to initially establish a base for assessing the performance of the Eventspotter. Second, the synthetic annotation was made available for training the Stanford CRF. We will see during the presentation of the results that the Stanford CRF performed quite differently with synthetically annotated training data when compared to manually annotated training data. Third, we realised by observation, that some aspect of the synthetic annotation approach could be applied during the manual annotation process.
	In the case of manual annotation, we refrained from annotating those spots where the event title does not directly refer to the event itself. For instance in the phrase ``Carrie Underwood announces her North American tour Blown Away'', `Carrie Underwood' refers to the artist and hence is not annotated. However, by doing these the human annotators realised that there was ambiguity with respect to event titles. Many events were represented as just artists or a combination of artists and locations which led to the problem of missing annotations. Therefore, in addition to refraining from annotating spurious event titles, we took into consideration the event titles in the Eventmedia dataset which were used to rectify any such cases. It made sense to not rely solely on the annotatorsâ€™ knowledge, but infact, partially adopt the synthetic annotation approach in order to generate a more robust golden dataset.
\addcontentsline{toc}{section}{Golden Dataset Creation}

\section*{Results}

The Eventspotter performed better than the Stanford CRF in terms of recall and F-measure. It obtained a score of 89.26\% for recall and 72.73\% for F-measure, over the scores of 4.27\% and 8\% respectively of the Stanford CRF trained with synthetic annotation and 54.70\% and 60.95\% respectively when trained with manual annotation. To highlight once again, these set of tests on the synthetically annotated golden dataset were carried out merely to give ourselves an idea about the Eventspotter's performance. Opencalais did not identify any musical event entities at all. Opencalais detects events from various categories which does not include musical festival or concert. Since it does come close with the category `musical bands' we thought it would be interesting to note how many events it would detect. 
\begin{table}[ht]
\caption{Tests Performed with Synthetically Annotated Golden Dataset} % title of Table
\centering % used for centering table
\begin{tabular}{c c c c} % centered columns (4 columns)
\hline\hline %inserts double horizontal lines
Approach & Precision & Recall & F-Measure \\ [0.5ex] % inserts table 
%heading
\hline % inserts single horizontal line
Eventspotter & 61.36\% & \bf 89.26\% \bf & \bf 72.73\% \bf \\ % inserting body of the table
Stanford trained on synthetic data &  62.5\% & 4.27\% & 8\%\\
Stanford trained on manual data & \bf 68.82 \bf \% & 54.70\% & 60.95\%  \\
Opencalais & no events & no events & no events \\
\hline %inserts single line
\end{tabular}
\label{table:nonlin} % is used to refer this table in the text
\end{table}

The second set of tests run were run on the manually annotated golden dataset. Again the Eventspotter's performance outshined that of the Stanford CRF in terms of recall and F-measure. It obtained a score of 68.14\% for recall and 43.14\% for F-measure, over the scores of 54.87\% and 56.88\% respectively of the Stanford CRF trained with synthetic annotation and 11.5\% & 18.71\% respectively when trained with manual annotation. Again, Opencalais did not identify any musical event entities at all. We also tested a linear combination of Eventspotter and Stanford CRF trained on synthetically annotated data, with Stanford CRF acting as a gazateer to Eventspotter. Though there wasnt stark improvement, there was a slight increase in the recall score to 70.8\%. 
\begin{table}[ht]
\caption{Tests Performed with Manually Annotated Golden Dataset} % title of Table
\centering % used for centering table
\begin{tabular}{c c c c} % centered columns (4 columns)
\hline\hline %inserts double horizontal lines
Approach & Precision & Recall & F-Measure \\ [0.5ex] % inserts table 
%heading
\hline % inserts single horizontal line
Eventspotter & 31.56\% & \bf 68.14\% \bf & 43.14\% \\
Stanford trained on synthetic data  & \bf 59.05\%  \bf & 54.87\% & 56.88\%\\
Stanford trained on manual data & 50\% & 11.5\% & 18.71\%  \\
Stanford Gazeteer to Eventspotter & 31.25\% & \bf 70.8\% \bf & 43.36\% \\
Opencalais & no events & no events & no events \\
\hline %inserts single line
\end{tabular}
\label{table:nonlin} % is used to refer this table in the text
\end{table}










\addcontentsline{toc}{section}{Results}

\bibliography{biblio}{}
\end{document}
