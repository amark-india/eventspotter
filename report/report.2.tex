\documentclass[a4paper,11pt]{report}
\usepackage[latin1]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx} 
\usepackage{pdfpages}
\usepackage{fancyvrb}
\bibliographystyle{ieeetr}
\begin{document}

\begin{titlepage}
\begin{center}
\includegraphics[width=5cm]{EURECOM_logo_quadri}
\\[3cm]
\textbf{\Huge{Eventspotter}}
\\[0.2cm]
\textbf{\textsc{\LARGE{spotting entities that talk about events}}}
\\[2cm]
\textbf{\textsc{\LARGE{Project Report}}}
\\[0.5cm]
\LARGE{Amar Kadamalakunte , Meghana Sreekanta Murthy}
\\[0.1cm]
\large{Fall 2012 - Spring 2013}
\\[7.92cm]
\columnsep3cm
\begin{tabular}{p{8cm} p{8.5cm}}
\small{\textbf{Supervisors:}\newline
Rapha\"el Troncy\newline
Giuseppe Rizzo} 
&
\small{\textbf{EURECOM\newline Multimedia Department}}
\end{tabular}
\end{center}
\end{titlepage}


 \tableofcontents

\chapter*{1 Abstract}
\addcontentsline{toc}{chapter}{1 Abstract}
By definition, an event is an occurrence in the past, present or future. Events could be classified as musical events, sports events, news events, etc. The aim of our work is to develop an approach to spot the existence of musical event entities in free form text. The Eventspotter uses a large dataset called EventMedia to identify candidate event spots in the text, and disambiguates them by assigning a confidence score. The confidence score itself is based on the identification of correlated entities (event location or artists) in the surrounding text and cosine similarity measure between the surrounding text and existing description of event in the dataset. We used precision, recall and f-measure to evaluate our approach. We used event description from the Eventmedia dataset as input and found that the Eventspotter performed better than some exisiting approaches for event detection in the musical domain. We present our method and findings in this paper.

\chapter*{1 Introduction}
We first take a detailed look at the Eventspotter Approch and then in subsequent sections we speak at length about the golden data set creation, usage of 10-fold cross validation to evaluate the performance of the system and conclude with our thoughts on how we can potentially better the performance of the eventspotter with the help of machine learning techniques. 
\addcontentsline{toc}{chapter}{2 Introduction}

%\section*{Motivation for the project}
%\section*{Contents}

\chapter*{2 Related Work}
Many people have tried to tackle the problem of entity detection in the past. While some have relied purely on the linguistic properties of text others have used context and domain knowledge to accurately detect entities. \cite{Gruhl_contextand} was one such approach that inspired us during the literature survey phase of the Eventspotter. In this case, the utilisation of context and domain knowledge along with the application of real world constraints proved to be beneficial in improving the accuracy of entity detection. The main goal was to establish semantic relationships between entities in social networking and content sharing sites like MySpace and Youtube. In this approach, the authors used an edit- distance based spotter to match the knowledge base with the text taken from online forums discussing popular music. They then checked for domain and context information and applied an SVM classifier. Further, a study of \cite{Hassell06ontology-drivenautomatic} introduced us to the approach of identifying entities based on knowledge from populated ontologies such as DBWorld and DBLP. They used clues such as entity names, text-proximity, text co-ocurrence, popularity of entities, semantic relationships between entities for disambiguation. This approach further vindicated our idea of using the large Eventmedia knowledge base to detect events in Eventspotter.\newline

As part of another approach , DBpedia URIs were used for automatic annotation of texts.  The DBPedia Spotlight \cite{Mendes11dbpediaspotlight:} used an extensive ontology with 272 classes, called DBpedia, and also provided the flexibility of choosing domain of interest.  2 sources of textual references include : the lexicon created from the DBPedia graph (of labels, redirects, disambiguations) and wikilinks. The Eventspotter derives various facets of the DBpedia Spotlight approach including the 4 main stages : (1)Spot phrases that may indicate a mention of a DBpedia  resource, (2)Select candidates by spotted phrase-to-resources mapping, (3)Disambiguate by using the context around spotted phrase, (4)Configure the annotation by customization from user with  Resource Prominence, Topic Pertinence, Contextual Ambiguity, Disambiguation Confidence.We also studied two interesting methodologies that were applied to social media and wikipedia for detection of events. TWICAL \cite{Ritter_opendomain} focused on linguistic properties and temporal expressions to detect events, classified them using the context of surrounding phrases and finally ranked them according to strong associations with a unique date. The Wikipedia Live Monitor\cite{DBLP:journals/corr/abs-1303-4702} tracked edits on various linguistic versions of a single document on wikipedia and relied on cross-language, full-text search across social networks in order to evaluate the \cite{Hassell06ontology-drivenautomatic} plausability of the edits being events. \newline

Although the above mentioned approaches are effective in entity spotting, there are a few limitations with each. In \cite{Gruhl_contextand}, the authors cite the absence of a training corpus and regions of text being commonly used words, as limiting factors. They  also speak of the ambiguous nature of song titles being a major limitation. In  \cite{Hassell06ontology-drivenautomatic} the non-exhaustive nature of any knowledge base proves to be the biggest hurdle. The same is the case with the DBpedia spotlight approach defined in \cite{Mendes11dbpediaspotlight:}, where the authors do not speak of using linguistic tools such as a Conditional Random Field classifier to detect entities that are non existent in the knowledge base.Both TWICAL \cite{Ritter_opendomain} and Wikipedia Live monitor \cite{DBLP:journals/corr/abs-1303-4702} are dependant on social networking sites to detect events. As a result, events that are not publicised in the social networking domain go undetected. The fact that each of these approaches have limitations of their own, goes to show that there is still scope for improvement in the field of event detection. The EventSpotter project is an attempt to meet this need for a more complete approach towards event detection. 

\section*{2.2 BookSpotter}

BookSpotter Enhancement Engine by Sztaki\footnote{http://blog.iks-project.eu/introducing-bookspotter-enhancement-engine-by-sztaki/} takes a knowledge based approch to find book occurrences within text. It loads a list of titles into memory and matches them with tokens of input text to spot book occurrences. It further disambiguates each spot using the author string stored in a database. If one or more authors are present in the text, the spot is considered as a valid one. The BookSpotter enhancement engine is designed to work as a module inside Apache Stanbol\footnote{http://stanbol.apache.org/}. Apache Stanbol provides a set of reusable components for semantic content management. The BookSpotter uses Stanbol for presenting the detected entities as RDF. We were greatly inspired by the idea behind the BookSpotter project. We spent a considerable amount of time  analysing and understanding the logic of the BookSpotter system. In a way, the EventSpotter project stands upon the shoulders of BookSpotter. We supplement the logic of the BookSpotter with event-specific features such as presence of: artists, location, date etc.

\addcontentsline{toc}{section}{2.2 Bookspotter}

\addcontentsline{toc}{chapter}{2 Related Work}

\chapter*{3 Approach}
The Eventspotter approach has four stages namely: (1)Preprocessing, (2)Candidate selection, (3)Disambiguation and (4)Postprocessing. It is imperative to restate that we have reused components of the BookSpotter which act as a foundation on which the EventSpotter is buit on.
\section*{3.1 Overview}
Before we look at each of these in greater detail, it is important to first gain a high level understanding of the entire system.

[diagram]

\subsection*{3.1.1 EventMedia Dataset}
We used a snapshot of the EventMedia database which constituted 34195 events. This snapshot, taken on 26th November 2012, contains metadata for events that occurred during the year 2012. Each event in the eventmedia snapshot has the following attributes: eventId, eventTitle, publisher, date (in the form startDate, For instance 10-21-2012), location, LIST OF \textless categories\textgreater, LIST OF \textless agents\textgreater and event description. The events belonged to one or more of the following categories: Musical Concert, Festivals, Nightlife, Social Gathering, Performing Arts, Visual and Performing Arts, Movies, Visual Arts, Food, Neighborhood, Community, Religion and Spirituality, Museums and Attractions, Outdoors Recreation, Family and Kids, Galleries and Exhibits, Professional, Fundraising and Charity, Commercial and Sales, Education, Sports, Organizations and Meetups, Business and Networking, Politics, Media and Literary, Health and Wellness, Life Style, Conferences and Tradeshows, On Campus, Science, Technology, Animals, Comedy, Other and Miscellaneous.
\addcontentsline{toc}{subsection}{3.1.1 EventMedia Dataset}

\subsection*{3.1.2 EventTitles}
EventTitles.list is a text file which contains, as the name suggests, titles of all events listed in the EventMedia database, mapped to their corresponding eventId. This file is loaded onto memory at the beggining of the execution.
\addcontentsline{toc}{subsection}{3.1.2 Eventtitles}

As shown in the figure, the EventSpotter spots candidate entities by matching input text with the eventtitles file and then validates these spots using the agent and location fields from the eventMedia dataset.
\addcontentsline{toc}{section}{3.1 Overview}

\section*{3.2 Preprocessing}


We first follow a method of text tokenization. Our tokenizer divides the input text as well as a list of event title names into a number of distinct tokens. The tokenizer ensures that there is consistency when it is required to match any 2 strings between the two. Second, the entire input text is also divided into a a set of sentences, a process which we call `sentensize’. The objective of this process is to ensure the availability of distinct sentences for the later stage of Disambiguation.

\addcontentsline{toc}{section}{3.2 Preprocessing}

\section*{3.3 Candidate Selection}

For every input text token that was derived from the previous tokenization stage, our system checks for a case-insensitive match with tokens from the list of event title names. When a match is found, the string of tokens are added to a list of matches. Further, only if each match starts with capital letter and first character is not a digit, it is selected as a candidate.
\addcontentsline{toc}{section}{3.3 Candidate Selection}

\section*{3.4 Disambiguation}


\addcontentsline{toc}{section}{3.4 Disambiguation}



\addcontentsline{toc}{chapter}{3 Approach}
\chapter*{4 Evaluation}
We tested the Eventspotter against Stanford Conditional Random Field\footnote{\url{http://nlp.stanford.edu/software/CRF-NER.shtml}} and Opencalais\footnote{\url{http://www.opencalais.com/}}. These tests were of a relatively small scale due to the need to manually create the the golden dataset. We felt it would be interesting to understand the performance of the flexible knowledge based approach of the Eventspotter against the purely grammar-based approaches of Stanford and Opencalais. Before we present the results of our experiments, it would be appropriate to understand the creation of the golden dataset. 

\section*{4.1 Golden Dataset Creation}
We used 60 event descriptions for the creation of the golden dataset. 30 documents were known to include event titles as well as artist names while the remaining were chosen at random. We performed cross validation with factor k =10. We followed a set of rules in order to ensure consistent annotation for the golden dataset creation. For the purpose of annotation we defined a musical event title as : a ``proper noun that refers to a musical concert or festival''. In all cases, an agreement between 4 human taggers was necessary. Any disagreement was resolved after deliberation and discussion. A candidate spot, was tagged with \textless EVENT \textgreater  and \textless /EVENT\textgreater. For instance, if `scala' is an event in the phrase `scala:)', we annotate it as `\textless EVENT\textgreater scala \textless /EVENT\textgreater:)'. For experimental purposes, we followed 2 types of annotation : synthetic annotation and manual annotation . 
	In the case of synthetic annotation, we carried out a straightforward string comparison between the input text and all event titles in the Eventmedia dataset. If there was a match, the spot was annotated as an event. There were several reasons for doing so. First, we were able to initially establish a base for assessing the performance of the Eventspotter. Second, the synthetic annotation was made available for training the Stanford CRF. We will see during the presentation of the results that the Stanford CRF performed quite differently with synthetically annotated training data when compared to manually annotated training data. Third, we realised by observation, that some aspect of the synthetic annotation approach could be applied during the manual annotation process.
	In the case of manual annotation, we refrained from annotating those spots where the event title does not directly refer to the event itself. For instance in the phrase ``Carrie Underwood announces her North American tour Blown Away'', `Carrie Underwood' refers to the artist and hence is not annotated. However, by doing these the human annotators realised that there was ambiguity with respect to event titles. Many events were represented as just artists or a combination of artists and locations which led to the problem of missing annotations. Therefore, in addition to refraining from annotating spurious event titles, we took into consideration the event titles in the Eventmedia dataset which were used to rectify any such cases. It made sense to not rely solely on the annotators’ knowledge, but infact, partially adopt the synthetic annotation approach in order to generate a more robust golden dataset.
\addcontentsline{toc}{section}{4.1 Golden Dataset Creation}

\section*{4.2 Results}

The Eventspotter performed better than the Stanford CRF in terms of recall and F-measure. It obtained a score of 89.26\% for recall and 72.73\% for F-measure, over the scores of 4.27\% and 8\% respectively of the Stanford CRF trained with synthetic annotation and 54.70\% and 60.95\% respectively when trained with manual annotation. To highlight once again, these set of tests on the synthetically annotated golden dataset were carried out merely to give ourselves an idea about the Eventspotter's performance. Opencalais did not identify any musical event entities at all. Opencalais detects events from various categories which does not include musical festival or concert. Since it does come close with the category `musical bands' we thought it would be interesting to note how many events it would detect. 
\begin{table}[ht]
\caption{Tests Performed with Synthetically Annotated Golden Dataset} % title of Table
\centering % used for centering table
\begin{tabular}{c c c c} % centered columns (4 columns)
\hline\hline %inserts double horizontal lines
Approach & Precision & Recall & F-Measure \\ [0.5ex] % inserts table 
%heading
\hline % inserts single horizontal line
Eventspotter & 61.36\% & \bf 89.26\% \bf & \bf 72.73\% \bf \\ % inserting body of the table
Stanford trained on synthetic data &  62.5\% & 4.27\% & 8\%\\
Stanford trained on manual data & \bf 68.82 \bf \% & 54.70\% & 60.95\%  \\
Opencalais & no events & no events & no events \\
\hline %inserts single line
\end{tabular}
\label{table:nonlin} % is used to refer this table in the text
\end{table}

The second set of tests run were run on the manually annotated golden dataset. Again the Eventspotter's performance outshined that of the Stanford CRF in terms of recall and F-measure. It obtained a score of 68.14\% for recall and 43.14\% for F-measure, over the scores of 54.87\% and 56.88\% respectively of the Stanford CRF trained with synthetic annotation and 11.5\% & 18.71\% respectively when trained with manual annotation. Again, Opencalais did not identify any musical event entities at all. We also tested a linear combination of Eventspotter and Stanford CRF trained on synthetically annotated data, with Stanford CRF acting as a gazateer to Eventspotter. Though there wasnt stark improvement, there was a slight increase in the recall score to 70.8\%. 
\begin{table}[ht]
\caption{Tests Performed with Manually Annotated Golden Dataset} % title of Table
\centering % used for centering table
\begin{tabular}{c c c c} % centered columns (4 columns)
\hline\hline %inserts double horizontal lines
Approach & Precision & Recall & F-Measure \\ [0.5ex] % inserts table 
%heading
\hline % inserts single horizontal line
Eventspotter & 31.56\% & \bf 68.14\% \bf & 43.14\% \\
Stanford trained on synthetic data  & \bf 59.05\%  \bf & 54.87\% & 56.88\%\\
Stanford trained on manual data & 50\% & 11.5\% & 18.71\%  \\
Stanford Gazeteer to Eventspotter & 31.25\% & \bf 70.8\% \bf & 43.36\% \\
Opencalais & no events & no events & no events \\
\hline %inserts single line
\end{tabular}
\label{table:nonlin} % is used to refer this table in the text
\end{table}
\addcontentsline{toc}{section}{4.2Results}
\addcontentsline{toc}{chapter}{4 Evaluation}

\chapter*{5 Conclusion and Future work}

In this paper, we presented Eventspotter, a tool for detecting musical event entities in unstructured text. We brought to light the various existent ideas that inspired our approach for the Eventspotter. We then detailed the methodologies implemented followed by the evaluation results. We compared the Eventspotter with other open source services and obtained encouraging results that asserted our belief that there is scope for further research in the direction adopted by Eventspotter.  

In the future, we plan to incorporate into the Eventspotter logic, Support Vector Machine classifier and machine learning techniques using WAEKA. Parts of speech tagging, domain specific terms, sentimental expressions and results from the Stanford CRF can be used as features to train the SVM classifier. Also, we can maintain a `white-list` of verbs which are known occur in the proximity of event title in text. This list can be also used as feature to the SVM classifier. We plan to further extend our knowledge base to include other popular data sources such as DBpedia, DBLP, DBworld etc. We would like to explore the idea of generating.We hope that improvements such as these would be a stepping stone for the evolution of a high performance Eventspotter.
\addcontentsline{toc}{chapter}{5 Conclusion and Future work}
\bibliography{biblio}{}
\end{document}
